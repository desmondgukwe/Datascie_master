---
title: "Linked IN"
author: "Desmond Gukwe"
date: "11/12/2019"
output: html_document
---



```{r}
# load twitter library - the rtweet library is recommended now over twitteR
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# text mining library
library(tidytext)
# plotting packages
library(igraph)
library(ggraph)
library(sqldf)
library(ggplot2)
library(gganimate)
theme_set(theme_bw())
library(png)
library(gifski)
```

```{r}
eskom_tweets <- search_tweets(q = "#eskom", n = 100000000000,
                                    ,
                                      include_rts = FALSE)

h_tags <- paste0(eskom_tweets$hashtags)
df<- select (eskom_tweets,c('user_id', 'created_at','text','source','location','quoted_location','country','followers_count'))
df<-cbind(df,h_tags)
```

```{r}
library(pander)
x<-sqldf("select count(user_id) tweets,count(distinct user_id) users from df",drv = 'SQLite')
pander(x)
```

- 8609 tweets were created by 4787 users from the 4th to 13th of Dec ,which had a Eskom harsh tag

```{r}
df$created_at_2 <- as.Date(as.character(df$created_at))
```




```{r}
x<-sqldf("select created_at_2,count(user_id) tweets,count(distinct user_id) users 
         from df 
         group by 1",drv = 'SQLite')
pander(x)
```

```{r}

p <- ggplot(x, aes(created_at_2, tweets, fill = tweets)) +
  geom_col() +
  scale_fill_distiller(palette = "Reds", direction = 1) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    panel.grid.major.y = element_line(color = "white"),
    panel.ontop = TRUE
  )+ theme(axis.text.x = element_text(angle = 90, hjust = 1))+labs(x = "Day of Month", y = "Tweets",title = "Daily Number of Tweets with a hashtag #Eskom")

```


```{r}
p+transition_states(created_at_2, wrap = FALSE) +shadow_mark()
```

```{r}
x<-sqldf("select source,created_at_2 ,count(user_id)  tweets,count(distinct user_id) users 
         from df 
         group by 1,2",drv = 'SQLite')

```



```{r}
p <- ggplot(
  x, 
  aes(x = tweets, y=users, size = users, colour = source)
  ) +
  geom_point(show.legend = FALSE, alpha = 0.7) +
  scale_color_viridis_d() +
  scale_size(range = c(2, 12)) +
  scale_x_log10() +
  labs(x = "Tweets", y = "Users",title = "Daily Number of Tweets with a harsh tag #Eskom")
     

p + transition_time(created_at_2) +
  labs(title = "Date: {frame_time}")

```

```{r}
p + transition_time(created_at_2) +
  labs(title = "Date: {frame_time}") +
  view_follow(fixed_y = TRUE)
```

```{r}
x<-sqldf("select * , case when source like '%ipad%' or source like '%iphone%'  or source like '%mac%'  then 'iOs'
                          when source like '%web%' then 'web'
                          when source like '%android%' then 'android' end as top_sources
        from
        df where source like '%ipad%' 
        or source like '%iphone%' 
        or source like '%web%'
       or  source like '%android%'
         ",drv = 'SQLite')
x<-sqldf("select top_sources ,created_at_2,count(user_id) tweets,count(distinct user_id) users 
         from x 
         group by 1,2",drv = 'SQLite')
pander(x)
```

```{r}
 p <- ggplot(
  x, 
  aes(x = tweets, y=users, size = users, colour = top_sources)
  ) +
  geom_point(show.legend = FALSE, alpha = 0.7) +
  scale_color_viridis_d() +
  scale_size(range = c(2, 12)) +
  scale_x_log10() +
  labs(x = "Tweets", y = "Platform")
p
```

```{r}
p + transition_time(created_at_2) +
  labs(title = "created_at_2: {frame_time}")
```


```{r}
p <- ggplot(
  x,
  aes(created_at_2, users, color = factor(top_sources))
  ) +
  geom_line() +
  scale_color_viridis_d() +
  labs(x = "Day of Month", y = "Tweets",title = "Daily Number of Tweets with a hashtag #Eskom by top sources") +
  theme(legend.position = "top")
p
```

```{r}
p + 
  geom_point(aes(group = seq_along(created_at_2))) +
  transition_reveal(created_at_2)
```
```{r}
# Create a new data frame with only words
  TranslatedText <- as.vector(df$text)
  TranslatedText <- data_frame(line = 1:nrow(df), text = TranslatedText)
# Split reviews to individual words - "Tokenization"
  tidy_df <- TranslatedText %>%
    unnest_tokens(word, text)
  
# Remove stop words
  data(stop_words)
  
  tidy_df <- tidy_df %>%
    anti_join(stop_words)
  
  tidy_df %>%
    count(word, sort = TRUE)
```
```{r}
# Visualize words that occur +100 times
tidy_df %>%
count(word, sort = TRUE) %>%
  filter(n >1) %>%
  mutate(word = reorder(word, n))
tidy_df<-sqldf('select * from tidy_df where word not in ("https","t.co")')
```

```{r}
# Add sentiment scores to each word
  get_sentiments("bing") 
  
  bing_word_counts <- tidy_df %>%
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, sort = TRUE) %>%
  ungroup()
  
  
bing_word_counts %>%
    group_by(sentiment) %>%
    top_n(25) %>%
    ungroup() %>%
    mutate(word = reorder(word, n)) %>%
# Visualize the distrubution of word sentiment
  ggplot(aes(word, n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    theme_minimal()+
    labs(title="Distribution of word sentiment", subtitle = "Words that contribute to positive and negative sentiment", x="", y="Contribution to sentiment")+
    facet_wrap(~sentiment, scales = "free_y") +
  coord_flip()
```


```{r}
##install.packages("wordcloud") # word-cloud generator
library(reshape2)
library(wordcloud)
    
# Word cloud showing 200 words
 tidy_df %>%
    anti_join(stop_words) %>%
    count(word) %>%
    with(wordcloud(word, n, use.r.layout=FALSE,max.words = 200))

```

```{r}
# Create a new data frame with only words
  TranslatedText <- as.vector(df$h_tags)
  TranslatedText <- data_frame(line = 1:nrow(df), text = TranslatedText)
# Split reviews to individual words - "Tokenization"
  tidy_df <- TranslatedText %>%
    unnest_tokens(word, text)
  
# Remove stop words
  data(stop_words)
  
  tidy_df <- tidy_df %>%
    anti_join(stop_words)
  tidy_df<-sqldf('select * from tidy_df where word not like  "%%eskom%%" ')
  tidy_df %>%
    count(word, sort = TRUE)
  
  tidy_df %>%
count(word, sort = TRUE) %>%
  filter(n >50) %>%
  mutate(word = reorder(word, n)) %>%
 
 ggplot(aes(word, n)) +
  theme_minimal()+
  labs(title="Distirbution of Top 20 Hashtags ", subtitle = "Below are the  top 20 harsh tags being used along with the #Eskom Hashtag", x="", y="Frequency")+
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

```{r}
length(unique(eskom_tweets$quoted_location))
## [1] 311

eskom_tweets %>%
  ggplot(aes(quoted_location)) +
  geom_bar() + coord_flip() +
      labs(x = "Count",
      y = "Location",
      title = "Twitter users - unique locations ")


??ggmap
```
```{r}
cities_df <- as.data.frame(climate_tweets$location)
locations_df <- mutate_geocode(as.character( as.data.frame(climate_tweets$location)), place)
cities<-c('Delft','Haarlem','Antwerp')
??tribble()
cities<-as.data.frame(cities)
cities$cities<-as.character(cities$cities)
mutate_geocode(cities$cities,place)

??mutate_geocode
users %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location, n)) %>%
  top_n(20) %>%
  ggplot(aes(x = location, y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Count",
      y = "Location",
      title = "Where Twitter users are from - unique locations ")

str(geocode("waco texas", output = "all"))
```

```{r}
users %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location,n)) %>%
  na.omit() %>%
  top_n(20) %>%
  ggplot(aes(x = location,y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Location",
      y = "Count",
      title = "Twitter users - unique locations ")
```

```{r}
library(ggmap)
```








```{r}
head(climate_tweets$text)
```

```{r}
# remove urls tidyverse is failing here for some reason
# climate_tweets %>%
#  mutate_at(c("stripped_text"), gsub("http.*","",.))

# remove http elements manually
climate_tweets$stripped_text <- gsub("http.*","",  climate_tweets$text)
climate_tweets$stripped_text <- gsub("https.*","", climate_tweets$stripped_text)
```

```{r}
# remove punctuation, convert to lowercase, add id for each tweet!
climate_tweets_clean <- climate_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
```

```{r}
# plot the top 15 words -- notice any issues?
climate_tweets_clean %>%
  count(word, sort = TRUE) %>%
  top_n(30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets")
```

```{r}
# load list of stop words - from the tidytext package
data("stop_words")
# view first 6 words
```


```{r}
head(stop_words)
## # A tibble: 6 x 2
##   word      lexicon
##   <chr>     <chr>  
## 1 a         SMART  
## 2 a's       SMART
```


```{r}
## 3 able      SMART  
## 4 about     SMART  
## 5 above     SMART  
## 6 according SMART

nrow(climate_tweets_clean)
## [1] 247606

# remove stop words from your list of words
cleaned_tweet_words <- climate_tweets_clean %>%
  anti_join(stop_words)

# there should be fewer words now
nrow(cleaned_tweet_words)
## [1] 133584
```

```{r}
# plot the top 15 words -- notice any issues?
cleaned_tweet_words %>%
  count(word, sort = TRUE) %>%
  top_n(30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(y = "Count",
      x = "Unique words",
      title = "Count of unique words found in tweets",
      subtitle = "Stop words removed from the list")
```

```{r}
##library(devtools)
##install_github("dgrtwo/widyr")
library(widyr)
```

```{r}
climate_tweets_paired_words <- climate_tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 4)

climate_tweets_paired_words %>%
  count(paired_words, sort = TRUE)
```
```{r}
library(tidyr)
climate_tweets_separated_words <- climate_tweets_paired_words %>%
  separate(paired_words, c("word1", "word2"), sep = " ")

climate_tweets_filtered <- climate_tweets_separated_words %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
climate_words_counts <- climate_tweets_filtered %>%
  count(word1, word2, sort = TRUE)

head(climate_words_counts)
```
```{r}
library(igraph)
library(ggraph)

# plot climate change word network
# (plotting graph edges is currently broken)
climate_words_counts %>%
        filter(n >= 24) %>%
        graph_from_data_frame() %>%
        ggraph(layout = "fr") +
        # geom_edge_link(aes(edge_alpha = n, edge_width = n))
        # geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
        geom_node_point(color = "darkslategray4", size = 3) +
        geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
        labs(title = "Word Network: Tweets using the hashtag - Climate Change",
             subtitle = "Text mining twitter data ",
             x = "", y = "")

```

